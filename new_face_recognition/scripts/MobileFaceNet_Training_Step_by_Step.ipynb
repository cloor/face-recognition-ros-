{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileFaceNet_Training_Step_by_Step\n",
    "\n",
    "Now let's switch the gear towards the MobileFaceNet Training and Evaluation. In this tutorial, we will introduce how to build up the arcface loss function, the neural net training and the system evaluation in a step by step fashion   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "\n",
    "Since the facenet is intended to extract the face image to highly discriminative features, the challenge in feature learning is how to appropriately design a loss function that enhance discrimiative power. There are two leading approches. One is to directly learn the feature vector such as [Contrastive Loss](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf) and [Triplet Loss](https://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/1A_089_ext.pdf). The other is to deem face recognition as a classification problem to separate different identities. In this repo, we use one of the state-the-art algorithms [insightface](https://arxiv.org/pdf/1801.07698.pdf) as a multi-class classifier. Insightface can not only effectively separate the identities, but also allow strong intra-class compactness and inter-class discrepancy simultaneously. The MobileFaceNet training supervised by the insightface loss is described as below:\n",
    "\n",
    "<img src=\"images/ipy_pic/diagram.png\"  width=\"900\" style=\"float: left;\">\n",
    "<img src=\"images/ipy_pic/equation.png\"  width=\"400\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding feature vector dimension 'd' from MobileFaceNet is set as 512 in this work. The feature vector will go through a fully connected layer with weight 'W' in shape 'd x n' towards 'n' classes. In insightface algorithm, the feature vector and weight(bias = 0) will be normalized to get the angle between feature and weight. After adding angular margin penalty 'm' and feature scale 's', the logit will then go through the softmax function to obtain cross entropy loss. The weight 'W' here provides the centre for each class, enforcing higher similarity for intra-class samples and diversity for inter-class samples.\n",
    "\n",
    "[wujiwang](https://github.com/wujiyang/Face_Pytorch) presents a very good demo to illustrate the performance difference between traditional softmax loass and softmax+center loss. Arcface loss essentially outperforms the center loss. \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/ipy_pic/softmax.gif\"  width=\"300\" style=\"float: left;\"> </td>\n",
    "<td> <img src=\"images/ipy_pic/softmax_center.gif\"  width=\"300\" > </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can build up a class named 'Arcface' using Pytorch to calculate cos logit and then pass it to softmax function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Arcface(Module):\n",
    "    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n",
    "    def __init__(self, embedding_size=512, classnum=51332,  s=64., m=0.5):\n",
    "        super(Arcface, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "        # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.m = m # the margin value, default is 0.5\n",
    "        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.mm = self.sin_m * m  # issue 1\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "    def forward(self, embbedings, label):\n",
    "        # weights norm\n",
    "        nB = len(embbedings)\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0) # normalize for each column\n",
    "        # cos(theta+m)\n",
    "        cos_theta = torch.mm(embbedings,kernel_norm)\n",
    "        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n",
    "        cos_theta_2 = torch.pow(cos_theta, 2)\n",
    "        sin_theta_2 = 1 - cos_theta_2\n",
    "        sin_theta = torch.sqrt(sin_theta_2)\n",
    "        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\n",
    "        cond_v = cos_theta - self.threshold\n",
    "        cond_mask = cond_v <= 0\n",
    "        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\n",
    "        cos_theta_m[cond_mask] = keep_val[cond_mask]\n",
    "        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\n",
    "        idx_ = torch.arange(0, nB, dtype=torch.long)\n",
    "        output[idx_, label] = cos_theta_m[idx_, label]\n",
    "        output *= self.s # scale up in order to make softmax work, first introduced in normface\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training and evaluation data can be downloaded from [model zoo](https://github.com/deepinsight/insightface/wiki/Dataset-Zoo) provided by deepinsight. All face images have been aligned by MTCNN and cropped to 112x112. The MS1M-ArcFace which has 85742 classses with 5.8 million images is recommended for the MobileFaceNet training. The images are preprocessed before passing to MobileFaceNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS1M dataset size:  5822653 / 85742\n",
      "MS1M data loading as shape: torch.Size([128, 3, 112, 112])\n",
      "MS1M label loading as shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def img_loader(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            \n",
    "            img = cv2.imread(path)\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.stack([img] * 3, 2)\n",
    "            return img\n",
    "    except IOError:\n",
    "        print('Cannot load image ' + path)\n",
    "\n",
    "class MS1M(data.Dataset):\n",
    "    def __init__(self, root, file_list, transform=None, loader=img_loader):\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "        image_list = []\n",
    "        label_list = []\n",
    "        with open(file_list) as f:\n",
    "            img_label_list = f.read().splitlines()\n",
    "        for info in img_label_list:\n",
    "            image_path, label_name = info.split(' ')\n",
    "            image_list.append(image_path)\n",
    "            label_list.append(int(label_name))\n",
    "\n",
    "        self.image_list = image_list\n",
    "        self.label_list = label_list\n",
    "        self.class_nums = len(np.unique(self.label_list))\n",
    "        print(\"MS1M dataset size: \", len(self.image_list), '/', self.class_nums)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_list[index]\n",
    "        label = self.label_list[index]\n",
    "\n",
    "        img = self.loader(os.path.join(self.root, img_path))\n",
    "\n",
    "        # random flip with ratio of 0.5\n",
    "        flip = np.random.choice(2) * 2 - 1\n",
    "        if flip == 1:\n",
    "            img = cv2.flip(img, 1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])  # range [0.0, 1.0] -> [-1.0,1.0]\n",
    "\n",
    "root = 'data_set/faces_emore_images'\n",
    "file_list = 'data_set/faces_emore_images/faces_emore_align_112.txt'\n",
    "dataset = MS1M(root, file_list, transform=transform) \n",
    "\n",
    "trainloader = data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers=2, drop_last=False)\n",
    "for det in trainloader:\n",
    "    print('{} data loading as shape:'.format('MS1M'), det[0].shape)\n",
    "    print('{} label loading as shape:'.format('MS1M'), det[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall training process is demonstrated as below. SGD+Momentum optimization algorithm is applied with scheduled learning rate decay. Here we provide a demo to show how the training will proceed. For detailed training script, one can refer to the \"train.py\" in this repo. The accuracy will stay as zero for a while, be patient :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/11, Iters: 000100, loss: 27.6344, train_accuracy: 0.0000, time: 0.37 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000200, loss: 27.5277, train_accuracy: 0.0000, time: 0.37 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000300, loss: 27.9309, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000400, loss: 27.5351, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000500, loss: 27.3709, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000600, loss: 27.9678, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000700, loss: 27.6118, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000800, loss: 27.6302, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 000900, loss: 28.2122, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n",
      "Epoch 0/11, Iters: 001000, loss: 28.0151, train_accuracy: 0.0000, time: 0.38 s/iter, learning rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from face_model import Backbone, MobileFaceNet, Arcface\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MobileFaceNet(512).to(device)  \n",
    "margin = Arcface(embedding_size=512, classnum=85742,  s=32., m=0.5).to(device)\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer_ft = optim.SGD([\n",
    "    {'params': model.parameters(), 'weight_decay': 5e-4},\n",
    "    {'params': margin.parameters(), 'weight_decay': 5e-4}], lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones=[6, 8, 10], gamma=0.3) \n",
    "total_iters = 0\n",
    "total_epoch = 12\n",
    "for epoch in range(total_epoch):\n",
    "    # train model\n",
    "    exp_lr_scheduler.step()\n",
    "    model.train()     \n",
    "    since = time.time()\n",
    "    for det in trainloader: \n",
    "        img, label = det[0].to(device), det[1].to(device)\n",
    "        optimizer_ft.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            raw_logits = model(img)\n",
    "            output = margin(raw_logits, label)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer_ft.step()\n",
    "            \n",
    "            total_iters += 1\n",
    "            # print train information\n",
    "            if total_iters % 100 == 0:\n",
    "                # current training accuracy \n",
    "                _, preds = torch.max(output.data, 1)\n",
    "                total = label.size(0)\n",
    "                correct = (np.array(preds.cpu()) == np.array(label.data.cpu())).sum()                  \n",
    "                time_cur = (time.time() - since) / 100\n",
    "                since = time.time()\n",
    "\n",
    "                for p in  optimizer_ft.param_groups:\n",
    "                    lr = p['lr']\n",
    "                print(\"Epoch {}/{}, Iters: {:0>6d}, loss: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}\"\n",
    "                      .format(epoch, total_epoch-1, total_iters, loss.item(), correct/total, time_cur, lr))\n",
    "        if total_iters == 1000:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall training results are shown below. It is definiately not the best result to be reached. One can fine tune the learning rate hyperparameters or arcface loss parameters 's' or 'm' to drive a higher training accuracy.  \n",
    "<table><tr>\n",
    "<td> <img src=\"images/ipy_pic/loss_train.png\"  width=\"500\" style=\"float: left;\"> </td>\n",
    "<td> <img src=\"images/ipy_pic/accuracy_train.png\"  width=\"500\" > </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "Here are quite a lot of published dataset that we can utilize to evaluate our system. In this repo, we use LFW, AgeDB-30 and CFP-FP to evalute the training results. These dataset provide the pair matching paradigm: given a pair of face images, decide whether the images are of the same person. LFW (labelled faces in the wild) contains 13233 web-collected images from 5749 identities with large variations in pose, exosure and illuminations. AgeDB-30 (Age Database) is a dataset with additional variations in age. The minimum and maximum ages are 3 and 101, therefore more challenging. CFP-FP (Celebrities in Frontal Profile) consists of frontal and profile images, thus the most challenging database. \n",
    "\n",
    "The evaluation result in this work is shown as below table. 'Flip' the image can be applied to encode the embedding feature vector with ~ 0.07% higer accuracy. L2 distance score slightly outperforms cos similarity (not necessarily the truely trend for other cases, but it is what we conclude in this work) \n",
    "\n",
    "\n",
    "|  Eval Type     |   Score   |   LFW   | AgeDB-30 | CFP-FP \n",
    "|:--------------:|:---------:|:-------:|:--------:|:-------\n",
    "|Flip            |  L2       |  99.52  |   96.30  |  92.93    \n",
    "|Flip            |  Cos      |  99.50  |   96.18  |  92.84   \n",
    "|UnFlip          |  L2       |  99.45  |   95.63  |  93.10   \n",
    "|UnFlip          |  Cos      |  99.45  |   95.65  |  93.10     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed evaluation code can be found from \"Evaluation.py\". The code is to extract all the feature vectors from each pair of images, separete the dataset into folds, find out the best threshold for 9 out of 10 folds and evaluate the accuracy for the last one. The process is repeated and averaged accuracy is calculated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 accuracy: 99.33 threshold: 1.4237\n",
      "2 accuracy: 99.83 threshold: 1.4228\n",
      "3 accuracy: 100.00 threshold: 1.4213\n",
      "4 accuracy: 99.33 threshold: 1.4214\n",
      "5 accuracy: 98.67 threshold: 1.4244\n",
      "6 accuracy: 99.67 threshold: 1.4220\n",
      "7 accuracy: 98.67 threshold: 1.4206\n",
      "8 accuracy: 100.00 threshold: 1.4282\n",
      "9 accuracy: 100.00 threshold: 1.4229\n",
      "10 accuracy: 99.67 threshold: 1.4227\n",
      "--------\n",
      "Average Acc:99.5167 Average Threshold:1.4230\n"
     ]
    }
   ],
   "source": [
    "from Evaluation import getFeature, evaluation_10_fold\n",
    "from data_set.dataloader import LFW, CFP_FP, AgeDB30\n",
    "\n",
    "detect_model = MobileFaceNet(512).to(device) \n",
    "detect_model.load_state_dict(\n",
    "            torch.load('Weights/MobileFace_Net', map_location=lambda storage, loc: storage))\n",
    "detect_model.eval()\n",
    "\n",
    "root = 'data_set/LFW/lfw_align_112'\n",
    "file_list = 'data_set/LFW/pairs.txt'\n",
    "dataset = LFW(root, file_list, transform=transform)\n",
    "dataloader = data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers=2, drop_last=False)\n",
    "featureLs, featureRs = getFeature(detect_model, dataloader, device, flip = True)\n",
    "ACCs, threshold = evaluation_10_fold(featureLs, featureRs, dataset, method = 'l2_distance')\n",
    "    \n",
    "for i in range(len(ACCs)):\n",
    "    print('{} accuracy: {:.2f} threshold: {:.4f}'.format(i+1, ACCs[i] * 100, threshold[i]))\n",
    "print('--------')\n",
    "print('Average Acc:{:.4f} Average Threshold:{:.4f}'.format(np.mean(ACCs) * 100, np.mean(threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
